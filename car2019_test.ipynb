{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following this tutorial: [Automated Keyword Extraction from Articles using NLP](https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34)\n",
    "\n",
    "#### next steps:\n",
    "* manual keywords\n",
    "* determining what keywords are\n",
    "look at nouns, get rid of stop words, see what pops out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up libraries we'll need\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>name</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>location_room</th>\n",
       "      <th>start_date_clean</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>pre_reg_flag</th>\n",
       "      <th>paid_flag</th>\n",
       "      <th>laptop_flag</th>\n",
       "      <th>speakers_cleaned</th>\n",
       "      <th>session_type</th>\n",
       "      <th>keywords</th>\n",
       "      <th>skill_level</th>\n",
       "      <th>session_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4178</td>\n",
       "      <td>(Generally) painless collaboration with the gr...</td>\n",
       "      <td>Traditional reporters and editors often view t...</td>\n",
       "      <td>Salon A&amp;B</td>\n",
       "      <td>2019-03-09</td>\n",
       "      <td>2019-03-09 15:30:00</td>\n",
       "      <td>2019-03-09 16:30:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Ryann Grochowski Jones, ProPublica (moderator)...</td>\n",
       "      <td>Panel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General interest</td>\n",
       "      <td>(Generally) painless collaboration with the gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4162</td>\n",
       "      <td>25th CAR: What a ride it's been!</td>\n",
       "      <td>Buckle up for a fast-paced ride through 25 yea...</td>\n",
       "      <td>Salon C&amp;D</td>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>2019-03-08 15:30:00</td>\n",
       "      <td>2019-03-08 16:30:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Doug Haddix, IRE/NICAR; Shawn McIntosh, Atlant...</td>\n",
       "      <td>Panel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General interest</td>\n",
       "      <td>25th CAR: What a ride it's been!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4189</td>\n",
       "      <td>50 databases to request right now</td>\n",
       "      <td>Get your FOI templates ready to roll. In this ...</td>\n",
       "      <td>Salon D</td>\n",
       "      <td>2019-03-09</td>\n",
       "      <td>2019-03-09 14:15:00</td>\n",
       "      <td>2019-03-09 15:15:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Mark Walker, The New York Times; Kate Martin, ...</td>\n",
       "      <td>Panel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>General interest</td>\n",
       "      <td>50 databases to request right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4198</td>\n",
       "      <td>A conversation with James B. Steele: Insights ...</td>\n",
       "      <td>This special session features the wit and wisd...</td>\n",
       "      <td>Salon A&amp;B</td>\n",
       "      <td>2019-03-09</td>\n",
       "      <td>2019-03-09 10:15:00</td>\n",
       "      <td>2019-03-09 11:15:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Sarah Cohen, ASU Cronkite School of Journalism...</td>\n",
       "      <td>Panel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A conversation with James B. Steele: Insights ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4301</td>\n",
       "      <td>Adding a text editor to your CAR toolkit</td>\n",
       "      <td>A good text editor is an essential tool for da...</td>\n",
       "      <td>Salon A&amp;B</td>\n",
       "      <td>2019-03-10</td>\n",
       "      <td>2019-03-10 10:15:00</td>\n",
       "      <td>2019-03-10 11:15:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Agustin Armendariz, The New York Times</td>\n",
       "      <td>Demo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Adding a text editor to your CAR toolkit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id                                               name  \\\n",
       "0      4178  (Generally) painless collaboration with the gr...   \n",
       "1      4162                   25th CAR: What a ride it's been!   \n",
       "2      4189                  50 databases to request right now   \n",
       "3      4198  A conversation with James B. Steele: Insights ...   \n",
       "4      4301           Adding a text editor to your CAR toolkit   \n",
       "\n",
       "                                   clean_description location_room  \\\n",
       "0  Traditional reporters and editors often view t...     Salon A&B   \n",
       "1  Buckle up for a fast-paced ride through 25 yea...     Salon C&D   \n",
       "2  Get your FOI templates ready to roll. In this ...       Salon D   \n",
       "3  This special session features the wit and wisd...     Salon A&B   \n",
       "4  A good text editor is an essential tool for da...     Salon A&B   \n",
       "\n",
       "  start_date_clean           start_time             end_time  pre_reg_flag  \\\n",
       "0       2019-03-09  2019-03-09 15:30:00  2019-03-09 16:30:00         False   \n",
       "1       2019-03-08  2019-03-08 15:30:00  2019-03-08 16:30:00         False   \n",
       "2       2019-03-09  2019-03-09 14:15:00  2019-03-09 15:15:00         False   \n",
       "3       2019-03-09  2019-03-09 10:15:00  2019-03-09 11:15:00         False   \n",
       "4       2019-03-10  2019-03-10 10:15:00  2019-03-10 11:15:00         False   \n",
       "\n",
       "   paid_flag  laptop_flag                                   speakers_cleaned  \\\n",
       "0      False        False  Ryann Grochowski Jones, ProPublica (moderator)...   \n",
       "1      False        False  Doug Haddix, IRE/NICAR; Shawn McIntosh, Atlant...   \n",
       "2      False        False  Mark Walker, The New York Times; Kate Martin, ...   \n",
       "3      False        False  Sarah Cohen, ASU Cronkite School of Journalism...   \n",
       "4      False        False             Agustin Armendariz, The New York Times   \n",
       "\n",
       "  session_type keywords       skill_level  \\\n",
       "0        Panel      NaN  General interest   \n",
       "1        Panel      NaN  General interest   \n",
       "2        Panel      NaN  General interest   \n",
       "3        Panel      NaN               NaN   \n",
       "4         Demo      NaN      Intermediate   \n",
       "\n",
       "                                       session_title  \n",
       "0  (Generally) painless collaboration with the gr...  \n",
       "1                   25th CAR: What a ride it's been!  \n",
       "2                  50 databases to request right now  \n",
       "3  A conversation with James B. Steele: Insights ...  \n",
       "4           Adding a text editor to your CAR toolkit  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and preview dataset\n",
    "# dataset from https://www.ire.org/events-and-training/conferences/nicar-2019\n",
    "dataset = pd.read_csv('data/car19guide.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>conference_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Generally) painless collaboration with the gr...</td>\n",
       "      <td>Traditional reporters and editors often view t...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25th CAR: What a ride it's been!</td>\n",
       "      <td>Buckle up for a fast-paced ride through 25 yea...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50 databases to request right now</td>\n",
       "      <td>Get your FOI templates ready to roll. In this ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A conversation with James B. Steele: Insights ...</td>\n",
       "      <td>This special session features the wit and wisd...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adding a text editor to your CAR toolkit</td>\n",
       "      <td>A good text editor is an essential tool for da...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  (Generally) painless collaboration with the gr...   \n",
       "1                   25th CAR: What a ride it's been!   \n",
       "2                  50 databases to request right now   \n",
       "3  A conversation with James B. Steele: Insights ...   \n",
       "4           Adding a text editor to your CAR toolkit   \n",
       "\n",
       "                                   clean_description conference_year  \n",
       "0  Traditional reporters and editors often view t...            2019  \n",
       "1  Buckle up for a fast-paced ride through 25 yea...            2019  \n",
       "2  Get your FOI templates ready to roll. In this ...            2019  \n",
       "3  This special session features the wit and wisd...            2019  \n",
       "4  A good text editor is an essential tool for da...            2019  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dataset with only the fields we want\n",
    "subset = dataset.loc[:, ['name','clean_description']] # 'session_title' seems to be same as name\n",
    "# add a column with the year\n",
    "subset['conference_year'] = '2019'\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through manual analysis of the schedule, I found some that are not classes or are duplicate sessions\n",
    "# these rows have names containing strings we can filter out using pipe\n",
    "# thanks to https://stackoverflow.com/questions/11350770/select-by-partial-string-from-a-pandas-dataframe\n",
    "\n",
    "filter_out = ['registration', 'sales', 'repeat']\n",
    "filtered_subset = subset[~subset['name'].str.contains('|'.join(filter_out))]\n",
    "# reset indices\n",
    "filtered_subset = filtered_subset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_description</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Traditional reporters and editors often view t...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buckle up for a fast-paced ride through 25 yea...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Get your FOI templates ready to roll. In this ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This special session features the wit and wisd...</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A good text editor is an essential tool for da...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   clean_description  word_count\n",
       "0  Traditional reporters and editors often view t...          68\n",
       "1  Buckle up for a fast-paced ride through 25 yea...          99\n",
       "2  Get your FOI templates ready to roll. In this ...          56\n",
       "3  This special session features the wit and wisd...         194\n",
       "4  A good text editor is an essential tool for da...          49"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preliminary text exploration\n",
    "# fetch word count for each description\n",
    "pd.options.mode.chained_assignment = None # get rid of warning...\n",
    "\n",
    "filtered_subset['word_count'] = filtered_subset['clean_description'].apply(lambda x: len(str(x).split(\" \")))\n",
    "filtered_subset[['clean_description','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    225.000000\n",
       "mean      81.973333\n",
       "std       37.831383\n",
       "min        3.000000\n",
       "25%       58.000000\n",
       "50%       75.000000\n",
       "75%       98.000000\n",
       "max      210.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics of word counts\n",
    "filtered_subset.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy descriptions to new column for pre-processing\n",
    "filtered_subset['preproc_desc'] = filtered_subset['clean_description']\n",
    "\n",
    "# # make every word in descriptions lowercase\n",
    "# filtered_subset['preproc_desc'] = filtered_subset['preproc_desc'].apply(lambda x: x.lower())\n",
    "# # remove punctuation before looking for common/uncommon words because adjacent punctuation changes words\n",
    "# filtered_subset['preproc_desc'] = filtered_subset['preproc_desc'].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "# filtered_subset['preproc_desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and        774\n",
       "to         662\n",
       "the        548\n",
       "a          397\n",
       "of         337\n",
       "data       336\n",
       "for        313\n",
       "you        298\n",
       "this       279\n",
       "in         279\n",
       "is         243\n",
       "how        224\n",
       "with       218\n",
       "will       207\n",
       "your       184\n",
       "session    174\n",
       "that       140\n",
       "on         120\n",
       "be         117\n",
       "or         112\n",
       "can        110\n",
       "are        100\n",
       "good        91\n",
       "well        86\n",
       "who         81\n",
       "it          77\n",
       "from        75\n",
       "as          74\n",
       "have        74\n",
       "some        74\n",
       "what        73\n",
       "learn       73\n",
       "use         67\n",
       "an          67\n",
       "using       66\n",
       "stories     61\n",
       "we          60\n",
       "but         60\n",
       "into        59\n",
       "about       57\n",
       "at          57\n",
       "class       55\n",
       "people      55\n",
       "we’ll       52\n",
       "tools       49\n",
       "more        49\n",
       "if          47\n",
       "get         46\n",
       "their       44\n",
       "—           44\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify common words\n",
    "# could be used for custom stop word list\n",
    "freq = pd.Series(' '.join(filtered_subset['preproc_desc']).split()).value_counts()[:50]\n",
    "freq\n",
    "# only domain-specific words that we might want to keep out of stoplist are 'data', 'learn', 'stories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "internship             1\n",
       "closer                 1\n",
       "marketingpromotions    1\n",
       "digitalfirst           1\n",
       "lesserknown            1\n",
       "mckinley               1\n",
       "nittygritty            1\n",
       "designers              1\n",
       "platform               1\n",
       "markets                1\n",
       "defraud                1\n",
       "publishable            1\n",
       "crosstabulations       1\n",
       "secondary              1\n",
       "recruiting             1\n",
       "character              1\n",
       "analytics              1\n",
       "parental               1\n",
       "stops                  1\n",
       "hours                  1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify uncommon words\n",
    "# inform cleaning needed?\n",
    "unfreq =  pd.Series(' '.join(filtered_subset \n",
    "         ['preproc_desc']).split()).value_counts()[-20:]\n",
    "unfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for text-preprocessing\n",
    "\n",
    "# download these the first time you run this\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet') \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# stemming normalizes text by removing suffixes\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# lemmatisation works based on the root of the word.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of stop words (plus adding custom stopwords if we want)\n",
    "stop_words = set(stopwords.words(\"english\"))# creating a list of custom stopwords\n",
    "new_words = []\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the stopwords, clean and normalize the corpus\n",
    "corpus = []\n",
    "for i in range(0, filtered_subset['preproc_desc'].count()): # don't hard code number of rows!\n",
    "    #Remove punctuations\n",
    "    text = re.sub('[^a-zA-Z]', ' ', filtered_subset['preproc_desc'][i])\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    ##Stemming\n",
    "    ps=PorterStemmer()    #Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buckle fast paced ride year data journalism told people drove car mainstream investigative reporting hear pivotal moment bizarre twist befuddled bureaucrat know hit featuring special guest expected guest speaker include crina boros center investigative journalism sarah cohen asu walter cronkite school journalism steve doig asu walter cronkite school journalism jaimi dowdell reuters mark horvit university missouri brant houston university illinois clarence jones independent journalist jennifer lafleur investigative reporting workshop james b steele independent journalist\n",
      "\n",
      "much openrefine clustering faceting feature session deep dive grel openrefine expression language equivalent excel formula thorough introduction grel syntax review common function explore clean dataset function covered session include replace split concatenate string comparison cell cross join multiple project together foreach session good people familiar openrefine least excel experience introduction openrefine check scheduled demo session\n"
     ]
    }
   ],
   "source": [
    "# view an example corpus item\n",
    "print(corpus[1])\n",
    "print()\n",
    "print(corpus[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skill level intermediate learn use tidyverse collection r package help make data journalism efficient stronger fun learn import clean analyze plot data story used package like dplyr tidyr readr ggplot tibble purr would like learn work together class preregistration required seating limited laptop provided training workshop prerequisite comfortable working r rstudio also familiar basic data analysis\n",
      "researcher stanford university collected examined record million local police stop city using programming language r learn analyze local policing data find pattern story session good people worked data r want learn analyze police data\n",
      "journalist pretty much coding experience love excel working spreadsheet attended ire nicar year feel ready make leap might little intimidated code trouble finding time learn new skill let talk challenge learning programming language whether python r sql overcome two self taught coder share journey lesson learned along way\n",
      "skill level intermediate charles minshew ire nicar introduce r free powerful open source programming language add statistical heft reporting end three hour session able take raw data import r start analysis topic include basic data importing working directory reading data installing package creating simple visualization clean explore sort data also talk find help stuck preregistration required seating limited laptop provided training workshop prerequisite session helpful comfortable working data ready take skill next level\n",
      "skill level intermediate hannah fresques sandhya kambhampati propublica along maryjo webster minneapolis star tribune introduce r free powerful open source programming language add statistical heft reporting end three hour session able take raw data import r start analysis topic include basic data importing working directory reading data installing package creating simple visualization clean explore sort data also talk find help stuck preregistration required seating limited laptop provided training workshop prerequisite session helpful comfortable working data ready take skill next level\n",
      "learn visualize data r introductory graphic class scatter plot bar chart box plot cover basic need get idea data telling using ggplot base r also cover labeling faceting legend cosmetic changing color line dot pattern displaying multiple plot one window session good r beginner want know visualize data\n",
      "r might sound intimidating programming language might surprised learn leap excel r big might think thanks rstudio interface lot simple use r package go pro con two software tool give tip make transition smooth possible decide go route comfortable excel experience r would like get taste diving hand class\n",
      "learn create beautiful static interactive map conduct geospatial analysis within r map sf leaflet package write script pull data shapefiles package utilize apis census transform analyze data turn exploratory map viz map nearly pretty enough publish session good people already familiarity r mapping census data using ggplot\n",
      "learn way around basic rstudio load basic package data analysis read data explore good class learn basic structure writing r code leave knowing get data r cleaning formatting task start basic analysis dataset give confidence take next step analysis session good beginner intermediate user\n",
      "learn use r collect information web page transform result usable data session also teach clean structure data analysis using tidyverse package session good people used r database software\n",
      "learn use r spot trend identify relationship data using social science theory method session use r statistical significance test cross tabulation linear regression session good anyone comfortable working spreadsheet database manager want learn basic statistical analysis experience r helpful\n",
      "basic linear regression great happens observation independent multiple observation per subject take statistical analysis skill next level learn measure relationship within correlated clustered data r using gee package session good intermediate r user comfortable linear logistic regression want learn complex modeling method\n",
      "save time produce better result create trusted analysis reduce risk error encourage collaboration implementing reproducible data analysis workflow technique going tool like rmarkdown interactive notebook weave together narrative text code produce formatted report sharing collaborator including future self walk hosting report raw data file github discus best practice structure project repos session good r user interested improving workflow\n",
      "sport one area struggle find data interested audience learn turn basic r interesting look game team season session good anyone interested r visualizing data sport previous experience r required\n",
      "want analyze theme sentiment complexity every state union address analyze member congress responded twitter realdonaldtrump session introduce tool needed tackle challenge text analysis r using tidytext package class good familiar basic tidyverse\n",
      "pretty good spreadsheet sure maybe bumping limit tool designed clean analyze data like start querying relational database scraping website making api call running statistical test building custom visuals help tell story describes might time learn code coding language right session newsroom coder speak popular programming language sql python r javascript discus common us show get started\n"
     ]
    }
   ],
   "source": [
    "sub = ' r '\n",
    "print('\\n'.join(s for s in corpus if sub in s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word counts for every single word\n",
    "every_count = pd.Series(' '.join(filtered_subset['preproc_desc']).split()).value_counts()[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
